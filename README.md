## ü§ñ Pr√©sentation du Projet

Ce projet impl√©mente un syst√®me avanc√© de simplification de texte utilisant l'apprentissage profond.

### üéØ Objectif Principal
R√©duire la complexit√© linguistique des textes tout en pr√©servant leur sens original, rendant l'information plus accessible √† un public plus large.

## üß† Architecture Technique

### Mod√®le
- Mod√®le T5 (Text-to-Text Transfer Transformer)
- PyTorch
- Hugging Face Transformers

### M√©triques d'√âvaluation
1. **SARI** (Simplification Awareness Reduced Information)
   - Mesure la qualit√© de simplification
   - √âvalue:
     * Mots conserv√©s
     * Mots supprim√©s
     * Mots ajout√©s

## üõ† Pr√©requis Techniques

### D√©pendances
- Python 3.8+
- PyTorch
- Transformers
- NLTK
- Evaluate
- sacrebleu sacremoses

### Installation
```bash
pip install torch transformers nltk evaluate sacrebleu sacremoses
```

## üîç D√©tails 

### Preprocessing
- Tokenization (auto)
- Troncature (r√©duction de la longueur)
- Padding (uniformiser la longueur des s√©quences dans un lot (batch) )

### Optimisations
- Gradient Checkpointing : Technique pour r√©duire l'utilisation de la m√©moire GPU
- Mixed Precision Training : Technique d'entra√Ænement qui utilise √† la fois des pr√©cisions de calcul 16 et 32 bits
- Adaptive Learning Rate : Adaptation du taux d'apprentissage

## üß© Personnalisation

### Param√®tres Configurables
- Mod√®le de base
- Taille du mod√®le
- Hyperparam√®tres d'entra√Ænement
- M√©triques d'√©valuation
---
